{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# inputs and target outputs\n",
    "\n",
    "X = np.asarray([[0.0,0.0],[0.0,1.0],[1.0,0.0],[1.0,1.0]])\n",
    "y = np.asarray([0.0,1.0,1.0,0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# layers\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def sig(x):\n",
    "    return 1.0/(1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Forward Calculation\n",
    "\n",
    "$\\mathbf{z_1} = \\mathbf{X}\\mathbf{w_1} + b_1$\n",
    "\n",
    "$\\mathbf{h_1} = relu(\\mathbf{z_1})$\n",
    "\n",
    "$\\mathbf{z_2} = \\mathbf{X}\\mathbf{w_2} + b_2$\n",
    "\n",
    "$\\mathbf{h_2} = relu(\\mathbf{z_2})$\n",
    "\n",
    "$\\mathbf{H} = [h_1, h_2]$\n",
    "\n",
    "$\\mathbf{z_3} = \\mathbf{H}\\mathbf{w_3} + b_3$\n",
    "\n",
    "$\\mathbf{y'} = \\sigma(z_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Grad Calculation\n",
    "\n",
    "$L = -\\frac{1}{n}[\\mathbf{y}^\\intercal\\ln(\\mathbf{y'}) + (1 - \\mathbf{y})^\\intercal\\ln{(1 - \\mathbf{y'})}]$\n",
    "\n",
    "$\\nabla_{w_3} L = (\\frac{\\partial z_3}{\\partial w_3})^\\intercal(\\frac{\\partial y'}{\\partial z_3})^\\intercal\\nabla_{y'} L$\n",
    "\n",
    "$\\nabla_{w_1} L = (\\frac{\\partial z_1}{\\partial w_1})^\\intercal(\\frac{\\partial h_1}{\\partial z_1})^\\intercal(\\frac{\\partial z_3}{\\partial h_1})^\\intercal(\\frac{\\partial y'}{\\partial z_3})^\\intercal\\nabla_{y'} L$\n",
    "\n",
    "$\\nabla_{w_2} L = (\\frac{\\partial z_2}{\\partial w_2})^\\intercal(\\frac{\\partial h_2}{\\partial z_2})^\\intercal(\\frac{\\partial z_3}{\\partial h_2})^\\intercal(\\frac{\\partial y'}{\\partial z_3})^\\intercal\\nabla_{y'} L$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_1} = (\\frac{\\partial z_1}{\\partial b_1})^\\intercal(\\frac{\\partial h_1}{\\partial z_1})^\\intercal(\\frac{\\partial z_3}{\\partial h_1})^\\intercal(\\frac{\\partial y'}{\\partial z_3})^\\intercal\\nabla_{y'} L$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_2} = (\\frac{\\partial z_2}{\\partial b_2})^\\intercal(\\frac{\\partial h_2}{\\partial z_2})^\\intercal(\\frac{\\partial z_3}{\\partial h_2})^\\intercal(\\frac{\\partial y'}{\\partial z_3})^\\intercal\\nabla_{y'} L$\n",
    "\n",
    "$\\frac{\\partial L}{\\partial b_3} = (\\frac{\\partial z_3}{\\partial b_3})^\\intercal(\\frac{\\partial y'}{\\partial z_3})^\\intercal\\nabla_{y'} L$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Individual Grad\n",
    "\n",
    "$\\nabla_{y'} L = -\\frac{1}{n}[\\frac{\\mathbf{y}}{\\mathbf{y'}} - \\frac{(1-\\mathbf{y})}{(1-\\mathbf{y'})}]$\n",
    "\n",
    "$\\frac{\\partial y'}{\\partial z_3} = diag(\\mathbf{y'}\\bigodot(1 - \\mathbf{y'}))$\n",
    "\n",
    "$\\frac{\\partial z_3}{\\partial h_1} = \\{\\mathbf{w}_{3,1}\\}^n$\n",
    "\n",
    "$\\frac{\\partial z_3}{\\partial h_2} = \\{\\mathbf{w}_{3,2}\\}^n$\n",
    "\n",
    "$\\frac{\\partial h_1}{\\partial z_1} = diag(min(1, \\mathbf{h_1}))$\n",
    "\n",
    "$\\frac{\\partial h_2}{\\partial z_2} = diag(min(1, \\mathbf{h_2}))$\n",
    "\n",
    "$\\frac{\\partial z_3}{\\partial w_3} = \\mathbf{H}$\n",
    "\n",
    "$\\frac{\\partial z_1}{\\partial w_1} = \\mathbf{X}$\n",
    "\n",
    "$\\frac{\\partial z_2}{\\partial w_2} = \\mathbf{X}$\n",
    "\n",
    "$\\frac{\\partial z_1}{\\partial b_1} = \\frac{\\partial z_2}{\\partial b_2} = \\frac{\\partial z_3}{\\partial b_3} = \\mathbf{1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Chain rule for vector case\n",
    "\n",
    "$\\nabla_{w_3} L = (\\frac{\\partial z_3}{\\partial w_3})^\\intercal(\\frac{\\partial y'}{\\partial z_3})^\\intercal\\nabla_{y'} L$\n",
    "\n",
    "A case like the previous one is calculated by the chain rule applied in reverse. As we want the result to be a column vector so we, effictively, transpose the gradient term, even if it is not explicitly annotated. Now, as the gradient is transposed, then the whole calculation needs to be transposed. The standard form of Jacobian is that the nominator is expanded in columns and denominator in rows. Now as the reverse is true for the transposed gradient, we need to transpose all the Jacobians, and reversing them of course, so that nominators and denominators are matched as the original chain rule.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# define net\n",
    "class Net:\n",
    "    def __init__(self):\n",
    "        \n",
    "        # init params\n",
    "        \n",
    "        self.w1 = rand.uniform(low=-1.0/np.sqrt(2), high=1.0/np.sqrt(2),size=(2))\n",
    "        self.w2 = rand.uniform(low=-1.0/np.sqrt(2), high=1.0/np.sqrt(2),size=(2))\n",
    "        self.w3 = rand.uniform(low=-1.0/np.sqrt(2), high=1.0/np.sqrt(2),size=(2))\n",
    "    \n",
    "        self.b1 = rand.uniform()\n",
    "        self.b2 = rand.uniform()\n",
    "        self.b3 = rand.uniform()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # forward propagation\n",
    "        \n",
    "        z = np.dot(x, self.w1) + self.b1\n",
    "        self.h1 = relu(z)\n",
    "        \n",
    "        z = np.dot(x, self.w2) + self.b2\n",
    "        self.h2 = relu(z)\n",
    "        \n",
    "        self.h = np.asarray([self.h1, self.h2]).transpose()\n",
    "        \n",
    "        z = np.dot(self.h, self.w3) + self.b3\n",
    "        self.o = sig(z)\n",
    "        \n",
    "        return self.o\n",
    "    \n",
    "    def train(self, x, y):\n",
    "        \n",
    "        # do forward\n",
    "        \n",
    "        lr = 0.9\n",
    "        \n",
    "        self.forward(x)\n",
    "        \n",
    "        n = x.shape[0]\n",
    "        \n",
    "        loss = -(1.0/n)*(np.dot(y, np.log(self.o)) + np.dot(1 - y, np.log(1 - self.o)))\n",
    "        \n",
    "        # calculate grad\n",
    "        \n",
    "        l_o_grad = -(1.0/n)*np.diag(((y/self.o) - ((1 - y)/(1 - self.o))))\n",
    "        o_z3_grad = np.diag(self.o * (1 - self.o))\n",
    "        z3_h1_grad = np.diag(np.full((n,), self.w3[0]))\n",
    "        z3_h2_grad = np.diag(np.full((n,), self.w3[1]))\n",
    "        h1_z1_grad = np.diag(np.minimum(1, self.h1))\n",
    "        h2_z2_grad = np.diag(np.minimum(1, self.h2))\n",
    "        z1_w1_grad = np.copy(x)\n",
    "        z2_w2_grad = np.copy(x)\n",
    "        z3_w3_grad = np.copy(self.h)\n",
    "        \n",
    "        l_z1_grad = h1_z1_grad.transpose().dot(z3_h1_grad.transpose()).dot(o_z3_grad.transpose()).dot(l_o_grad.transpose())\n",
    "        l_b1_grad = (1.0/n)*np.sum(np.ones((4,)).transpose().dot(l_z1_grad), axis=0)\n",
    "        l_w1_grad = (1.0/n)*np.sum(z1_w1_grad.transpose().dot(l_z1_grad), axis=1)\n",
    "\n",
    "        l_z2_grad = h2_z2_grad.transpose().dot(z3_h2_grad.transpose()).dot(o_z3_grad.transpose()).dot(l_o_grad.transpose())\n",
    "        l_b2_grad = (1.0/n)*np.sum(np.ones((4,)).transpose().dot(l_z2_grad), axis=0)\n",
    "        l_w2_grad = (1.0/n)*np.sum(z2_w2_grad.transpose().dot(l_z2_grad), axis=1)\n",
    "        \n",
    "        l_z3_grad = o_z3_grad.transpose().dot(l_o_grad.transpose())\n",
    "        l_b3_grad = (1.0/n)*np.sum(np.ones((4,)).transpose().dot(l_z3_grad), axis=0)\n",
    "        l_w3_grad = (1.0/n)*np.sum(z3_w3_grad.transpose().dot(l_z3_grad), axis=1)\n",
    "        \n",
    "        # update weights\n",
    "        \n",
    "        self.w1 = self.w1 - (lr * l_w1_grad)\n",
    "        self.b1 = self.b1 - (lr * l_b1_grad)\n",
    "        \n",
    "        self.w2 = self.w2 - (lr * l_w2_grad)\n",
    "        self.b2 = self.b2 - (lr * l_b2_grad)\n",
    "        \n",
    "        self.w3 = self.w3 - (lr * l_w3_grad)\n",
    "        self.b3 = self.b3 - (lr * l_b3_grad)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def params(self):\n",
    "        return (self.w1, self.b1, self.w2, self.b2, self.w3, self.b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.]: 0.664998496578\n",
      "[ 0.  1.]: 0.624808888412\n",
      "[ 1.  0.]: 0.667601856875\n",
      "[ 1.  1.]: 0.636321480932\n",
      "0.744869502657\n",
      "0.702058024645\n",
      "0.693644334337\n",
      "0.689672997975\n",
      "0.685760431266\n",
      "0.679985255619\n",
      "0.677611171674\n",
      "0.675966335924\n",
      "0.672659972483\n",
      "0.667100906468\n",
      "0.658185631099\n",
      "0.645869435936\n",
      "0.629711299855\n",
      "0.609088721388\n",
      "0.583463480496\n",
      "0.552614941759\n",
      "0.516839958402\n",
      "0.477058441141\n",
      "0.434754973099\n",
      "0.391740816312\n",
      "0.349811766717\n",
      "0.310435877187\n",
      "0.274579434537\n",
      "0.242694831925\n",
      "0.214820432336\n",
      "0.190720852545\n",
      "0.170015296239\n",
      "0.152271366339\n",
      "0.137063068631\n",
      "0.124001088355\n",
      "0.112744909067\n",
      "0.10300447711\n",
      "0.0945366003755\n",
      "0.0871391942147\n",
      "0.0806450695891\n",
      "0.0749160910536\n",
      "0.0698380424289\n",
      "0.0653162815532\n",
      "0.0612721432075\n",
      "0.057639999784\n",
      "0.0543648768529\n",
      "0.0514005257563\n",
      "0.0487078672957\n",
      "0.046253734269\n",
      "0.0440098536321\n",
      "0.0419520204672\n",
      "0.0400594254875\n",
      "0.0383141056049\n",
      "0.036700493349\n",
      "0.0352050459073\n",
      "0.0338159385028\n",
      "0.03252280994\n",
      "0.0313165506032\n",
      "0.0301891251337\n",
      "0.0291334235398\n",
      "0.0281431357109\n",
      "0.0272126452656\n",
      "0.0263369394353\n",
      "0.025511532293\n",
      "0.0247323991304\n",
      "0.0239959201817\n",
      "0.0232988322076\n",
      "0.0226381867162\n",
      "0.0220113138015\n",
      "0.0214157907555\n",
      "0.0208494147474\n",
      "0.0203101789789\n",
      "0.0197962518193\n",
      "0.0193059585018\n",
      "0.018837765027\n",
      "0.0183902639754\n",
      "0.0179621619721\n",
      "0.0175522685884\n",
      "0.0171594864937\n",
      "0.0167828026982\n",
      "0.0164212807519\n",
      "0.0160740537792\n",
      "0.0157403182501\n",
      "0.0154193283989\n",
      "0.0151103912141\n",
      "0.0148128619337\n",
      "0.0145261399877\n",
      "0.0142496653384\n",
      "0.0139829151725\n",
      "0.0137254009084\n",
      "0.0134766654842\n",
      "0.013236280895\n",
      "0.013003845956\n",
      "0.0127789842657\n",
      "0.01256134235\n",
      "0.012350587968\n",
      "0.0121464085654\n",
      "0.0119485098579\n",
      "0.011756614535\n",
      "0.0115704610706\n",
      "0.0113898026316\n",
      "0.0112144060745\n",
      "0.0110440510222\n",
      "0.0108785290142\n",
      "0.0107176427219\n",
      "[ 0.  0.]: 0.0168285016467\n",
      "[ 0.  1.]: 0.99548172724\n",
      "[ 1.  0.]: 0.995456270033\n",
      "[ 1.  1.]: 0.0160601540939\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "\n",
    "output = net.forward(X)\n",
    "\n",
    "for i,x in enumerate(X):\n",
    "    print(\"{}: {}\".format(x, output[i]))\n",
    "\n",
    "for i in range(1000):\n",
    "    loss = net.train(X, y)\n",
    "    \n",
    "    if i % 10 == 0:\n",
    "        print(loss)\n",
    "        \n",
    "output = net.forward(X)\n",
    "\n",
    "for i,x in enumerate(X):\n",
    "    print(\"{}: {}\".format(x, output[i]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
